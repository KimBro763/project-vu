Data Connection and Selections

Assumption:
There is a correlation between unemployment and its impact on mental health.

-------

Data collection and Connection:
For identifing underlying relation and mathematic proofs, we put togteher 2 main sources of data. 

1. Our data from Mental Health America's State of Mental Health in America yearly report. This is also the main sources behind our analysis and visulizations.
We pulled together YoY mental health population, population of suicide ideation by states.


2. From US Bureau of Labor to get the monthly unemployment rate by states from 2019 to 2020. 


---------

Data preparition for the machine learning model:

- All of our initial data was cleaned and stored in AWS RS service. I extracted the data live by connecting to the pgAdmin by sqlalchemy.
-We have 3 potetial tables based on the data sources. 

Given we our assumption is the relationship between unemployment data and mental health cases. We need to link this 2 seperate data sources toegtehr.
Method:
python merge.
- The tricky part is we need to use population for our analysis in order to be accurte with our stats. However, we have unrmploymrnt rate but not population.
- After the data connection, we used one of the table that contained mental health population and percentage by state to create the state population based on each year column.
Then, we merged the 2 tables of data on "year" and "state" columns.Last step was to create the "pop_unemployment" column for unemployment population based on the unemlpoyment rate
and state_population by year. 

So our master table for machine learning training and test is called new_df.

Before we getting into the complexity of machine learning, we want to quickly test out there is a strong correlationship between these data. So we used python pandas seaborn and numpy functions
to run a quick stats check.

between any_mental_health_pop and unemploment population, there is 0.87 correlation coefficient. 
Given the strong evidence of the data, we move on with our linear regression model.


<img unemployment_correlation>

Machine Learning

When combined the data, we need to ge rid of the null value caused by missing when we merged. 
When we done with the data merge:
Number of states left when there is no null valye for 
2019    51
2020    50
2021    48

First we want to make sure our data has a normal distribution.
using matplotlit histogram, we found out our data heavly skewed as the following:

<unnormalized data>

So our first step is to normalize the data by using numpy log10() function as the follwoing:

<normalized data>


From sklearn imported LinearRegression

Model Test_1:
We used a single year data of mental_health_cases and 2020 unemlpoyment data

<model_test1>

mean squared error (MSE): 0.07728
R-squared (R2 ): 0.746550
Model Score is: 0.728

Model_Tes2:
We introduced more data point to improve the model score.
<model_test2>

mean squared error (MSE): 0.06796
R-squared (R2 ): 0.7530
Model score: 0.769

Initial result: appraently, by introuding more data can help improve model accuracy.
- Our MSE also improved by 12%.
- Model accuracy score also improved significantly from 0.73 to 0.77


Model_Test3: OLS Model

In order to continue find the better moedl, we introduced ols model from statsmodels.formula.api .
This time, we imported preprocessing from sklearn to transform our data to more normalized table

